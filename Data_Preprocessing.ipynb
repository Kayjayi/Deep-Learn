{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_j2rbUwT8SQYrdngTZZuqEFx2zC77Qbh","timestamp":1684241395077},{"file_id":"1J6W_snjxbM1GKKwjGTU2Zib2V31Scy92","timestamp":1664564716234}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","<h2>Simple Regression Dataset - Straight Line</h2>\n","\n","Input Feature: X\n","\n","Target: 5*X + 8 + some noise\n","\n","Objective: Train a model to predict target for a given X\n","\n","# Straight Line Function\n","def straight_line(x):\n","    return 5*x + 8\n","\n","straight_line(25)\n","\n","straight_line(1.254)\n","\n","np.random.seed(5)\n","\n","samples = 150\n","x = pd.Series(np.arange(0,150))\n","y = x.map(straight_line) + np.random.randn(samples)*10\n","\n","df = pd.DataFrame({'x':x,'y':y})\n","\n","df.head()\n","\n","# Correlation will indicate how strongly features are related to the output\n","df.corr()\n","\n","plt.plot(df.x,df.y,label='Target')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.show()\n","\n","# Save all data\n","df.to_csv('linear_all.csv',index=False,\n","          columns=['x','y'])\n","\n","<h2>SageMaker Convention for Training and Validation files</h2>\n","\n","CSV File Column order: y_noisy, x\n","\n","Training, Validation files do not have a column header\n","\n","# Training = 70% of the data\n","# Validation = 30% of the data\n","# Randomize the datset\n","np.random.seed(5)\n","l = list(df.index)\n","np.random.shuffle(l)\n","df = df.iloc[l]\n","\n","df.head()\n","\n","rows = df.shape[0]\n","train = int(.7 * rows)\n","test = rows - train\n","\n","print(rows, train, test)\n","\n","# Write Training Set\n","df[:train].to_csv('linear_train.csv',index=False,header=False,columns=['y','x'])\n","\n","# Write Validation Set\n","df[train:].to_csv('linear_validation.csv',index=False,header=False,columns=['y','x'])\n","\n","\n","\n","<h2>Simple Regression Dataset - Linear Regression vs XGBoost</h2>\n","\n","Model is trained with XGBoost installed in notebook instance\n","\n","In the later examples, we will train using SageMaker's XGBoost algorithm.\n","\n","Training on SageMaker takes several minutes (even for simple dataset).\n","\n","If algorithm is supported on Python, we will try them locally on notebook instance\n","\n","This allows us to quickly learn an algorithm, understand tuning options and then finally train on SageMaker Cloud\n","\n","In this exercise, let's compare XGBoost and Linear Regression for simple regression dataset\n","\n","# Install xgboost in notebook instance.\n","#### Command to install xgboost\n","!pip install xgboost==1.2\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","\n","# XGBoost\n","import xgboost as xgb\n","# Linear Regression\n","from sklearn.linear_model import LinearRegression\n","\n","# All data\n","df = pd.read_csv('linear_all.csv')\n","\n","df.head()\n","\n","plt.plot(df.x,df.y,label='Target')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('Simple Regression Dataset')\n","plt.show()\n","\n","# Let's load Training and Validation Datasets\n","train_file = 'linear_train.csv'\n","validation_file = 'linear_validation.csv'\n","\n","# Specify the column names as the file does not have column header\n","df_train = pd.read_csv(train_file,names=['y','x'])\n","df_validation = pd.read_csv(validation_file,names=['y','x'])\n","\n","df_train.head()\n","\n","df_validation.head()\n","\n","plt.scatter(df_train.x,df_train.y,label='Training',marker='.')\n","plt.scatter(df_validation.x,df_validation.y,label='Validation',marker='.')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.title('Simple Regression Dataset')\n","plt.legend()\n","plt.show()\n","\n","X_train = df_train.iloc[:,1:] # Features: 1st column onwards\n","y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n","\n","X_validation = df_validation.iloc[:,1:]\n","y_validation = df_validation.iloc[:,0].ravel()\n","\n","# Create an instance of XGBoost Regressor\n","# XGBoost Training Parameter Reference:\n","#   https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n","regressor = xgb.XGBRegressor()\n","\n","# Default Options\n","regressor\n","\n","# Train the model\n","# Provide Training Dataset and Validation Dataset\n","# XGBoost reports training and validation error\n","regressor.fit(X_train,y_train, eval_set = [(X_train, y_train), (X_validation, y_validation)])\n","\n","# Get the Training RMSE and Evaluation RMSE\n","eval_result = regressor.evals_result()\n","\n","eval_result\n","\n","training_rounds = range(len(eval_result['validation_0']['rmse']))\n","\n","print(training_rounds)\n","\n","plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n","plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n","plt.grid(True)\n","plt.xlabel('Iterations')\n","plt.ylabel('RMSE')\n","plt.title('XGBoost Training Vs Validation Error')\n","plt.legend()\n","plt.show()\n","\n","xgb.plot_importance(regressor)\n","plt.show()\n","\n","## Validation Dataset Compare Actual and Predicted\n","\n","result = regressor.predict(X_validation)\n","\n","result[:5]\n","\n","plt.title('XGBoost - Validation Dataset')\n","plt.scatter(df_validation.x,df_validation.y,label='actual',marker='.')\n","plt.scatter(df_validation.x,result,label='predicted',marker='.')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","# RMSE Metrics\n","print('XGBoost Algorithm Metrics')\n","mse = mean_squared_error(df_validation.y,result)\n","print(\" Mean Squared Error: {0:.2f}\".format(mse))\n","print(\" Root Mean Square Error: {0:.2f}\".format(mse**.5))\n","\n","# Residual\n","# Over prediction and Under Prediction needs to be balanced\n","# Training Data Residuals\n","residuals = df_validation.y - result\n","plt.hist(residuals)\n","plt.grid(True)\n","plt.xlabel('Actual - Predicted')\n","plt.ylabel('Count')\n","plt.title('XGBoost Residual')\n","plt.axvline(color='r')\n","plt.show()\n","\n","# Count number of values greater than zero and less than zero\n","value_counts = (residuals > 0).value_counts(sort=False)\n","\n","print(' Under Estimation: {0}'.format(value_counts[True]))\n","print(' Over  Estimation: {0}'.format(value_counts[False]))\n","\n","# Plot for entire dataset\n","plt.plot(df.x,df.y,label='Target')\n","plt.plot(df.x,regressor.predict(df[['x']]) ,label='Predicted')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('XGBoost')\n","plt.show()\n","\n","## Linear Regression Algorithm\n","\n","lin_regressor = LinearRegression()\n","\n","lin_regressor.fit(X_train,y_train)\n","\n","Compare Weights assigned by Linear Regression.\n","\n","Original Function: 5*x + 8 + some noise\n","\n","\n","lin_regressor.coef_\n","\n","lin_regressor.intercept_\n","\n","result = lin_regressor.predict(df_validation[['x']])\n","\n","plt.title('LinearRegression - Validation Dataset')\n","plt.scatter(df_validation.x,df_validation.y,label='actual',marker='.')\n","plt.scatter(df_validation.x,result,label='predicted',marker='.')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","# RMSE Metrics\n","print('Linear Regression Metrics')\n","mse = mean_squared_error(df_validation.y,result)\n","print(\" Mean Squared Error: {0:.2f}\".format(mse))\n","print(\" Root Mean Square Error: {0:.2f}\".format(mse**.5))\n","\n","# Residual\n","# Over prediction and Under Prediction needs to be balanced\n","# Training Data Residuals\n","residuals = df_validation.y - result\n","plt.hist(residuals)\n","plt.grid(True)\n","plt.xlabel('Actual - Predicted')\n","plt.ylabel('Count')\n","plt.title('Linear Regression Residual')\n","plt.axvline(color='r')\n","plt.show()\n","\n","# Count number of values greater than zero and less than zero\n","value_counts = (residuals > 0).value_counts(sort=False)\n","\n","print(' Under Estimation: {0}'.format(value_counts[True]))\n","print(' Over  Estimation: {0}'.format(value_counts[False]))\n","\n","# Plot for entire dataset\n","plt.plot(df.x,df.y,label='Target')\n","plt.plot(df.x,lin_regressor.predict(df[['x']]) ,label='Predicted')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('LinearRegression')\n","plt.show()\n","\n","<h2>Input Features - Outside range used for training</h2>\n","\n","XGBoost Prediction has an upper and lower bound (applies to tree based algorithms)\n","\n","Linear Regression extrapolates\n","\n","# True Function\n","def straight_line(x):\n","    return 5*x + 8\n","\n","# X is outside range of training samples\n","X = np.array([-100,-5,160,1000,5000])\n","y = straight_line(X)\n","\n","df_tmp = pd.DataFrame({'x':X,'y':y})\n","df_tmp['xgboost']=regressor.predict(df_tmp[['x']])\n","df_tmp['linear']=lin_regressor.predict(df_tmp[['x']])\n","\n","df_tmp\n","\n","# XGBoost Predictions have an upper bound and lower bound\n","# Linear Regression Extrapolates\n","plt.scatter(df_tmp.x,df_tmp.y,label='Actual',color='r')\n","plt.plot(df_tmp.x,df_tmp.linear,label='LinearRegression')\n","plt.plot(df_tmp.x,df_tmp.xgboost,label='XGBoost')\n","plt.legend()\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.title('Input Outside Range')\n","plt.show()\n","\n","# X is inside range of training samples\n","X = np.array([0,1,3,5,7,9,11,15,18,125])\n","y = straight_line(X)\n","\n","df_tmp = pd.DataFrame({'x':X,'y':y})\n","df_tmp['xgboost']=regressor.predict(df_tmp[['x']])\n","df_tmp['linear']=lin_regressor.predict(df_tmp[['x']])\n","\n","df_tmp\n","\n","# XGBoost Predictions have an upper bound and lower bound\n","# Linear Regression Extrapolates\n","plt.scatter(df_tmp.x,df_tmp.y,label='Actual',color='r')\n","plt.plot(df_tmp.x,df_tmp.linear,label='LinearRegression')\n","plt.plot(df_tmp.x,df_tmp.xgboost,label='XGBoost')\n","plt.legend()\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.title('Input within range')\n","plt.show()\n","\n","<h2>Summary</h2>\n","\n","1. Use sagemaker notebook as your own server on the cloud\n","2. Install python packages\n","3. Train directly on SageMaker Notebook (for small datasets, it takes few seconds).\n","4. Once happy with algorithm and performance, you can train on sagemaker cloud (takes several minutes even for small datasets)\n","5. Not all algorithms are available for installation (for example: AWS algorithms like DeepAR are available only in SageMaker)\n","6. In this exercise, we installed XGBoost and compared performance of XGBoost model and Linear Regression\n","\n","\n","Qudratic Training\n","<h2>Quadratic Regression Dataset - Linear Regression vs XGBoost</h2>\n","\n","Model is trained with XGBoost installed in notebook instance\n","\n","In the later examples, we will train using SageMaker's XGBoost algorithm.\n","\n","Training on SageMaker takes several minutes (even for simple dataset).\n","\n","If algorithm is supported on Python, we will try them locally on notebook instance\n","\n","This allows us to quickly learn an algorithm, understand tuning options and then finally train on SageMaker Cloud\n","\n","In this exercise, let's compare XGBoost and Linear Regression for Quadratic regression dataset\n","\n","# Install xgboost in notebook instance.\n","#### Command to install xgboost\n","!pip install xgboost==1.2\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","\n","# XGBoost\n","import xgboost as xgb\n","# Linear Regression\n","from sklearn.linear_model import LinearRegression\n","\n","df = pd.read_csv('quadratic_all.csv')\n","\n","df.head()\n","\n","plt.plot(df.x,df.y,label='Target')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('Quadratic Regression Dataset')\n","plt.show()\n","\n","train_file = 'quadratic_train.csv'\n","validation_file = 'quadratic_validation.csv'\n","\n","# Specify the column names as the file does not have column header\n","df_train = pd.read_csv(train_file,names=['y','x'])\n","df_validation = pd.read_csv(validation_file,names=['y','x'])\n","\n","df_train.head()\n","\n","df_validation.head()\n","\n","plt.scatter(df_train.x,df_train.y,label='Training',marker='.')\n","plt.scatter(df_validation.x,df_validation.y,label='Validation',marker='.')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.title('Quadratic Regression Dataset')\n","plt.legend()\n","plt.show()\n","\n","X_train = df_train.iloc[:,1:] # Features: 1st column onwards\n","y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n","\n","X_validation = df_validation.iloc[:,1:]\n","y_validation = df_validation.iloc[:,0].ravel()\n","\n","# Create an instance of XGBoost Regressor\n","# XGBoost Training Parameter Reference:\n","#   https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n","regressor = xgb.XGBRegressor()\n","\n","regressor\n","\n","regressor.fit(X_train,y_train, eval_set = [(X_train, y_train), (X_validation, y_validation)])\n","\n","eval_result = regressor.evals_result()\n","\n","training_rounds = range(len(eval_result['validation_0']['rmse']))\n","\n","plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n","plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n","plt.grid(True)\n","plt.xlabel('Iteration')\n","plt.ylabel('RMSE')\n","plt.title('Training Vs Validation Error')\n","plt.legend()\n","plt.show()\n","\n","xgb.plot_importance(regressor)\n","plt.show()\n","\n","## Validation Dataset Compare Actual and Predicted\n","\n","result = regressor.predict(X_validation)\n","\n","result[:5]\n","\n","plt.title('XGBoost - Validation Dataset')\n","plt.scatter(df_validation.x,df_validation.y,label='actual',marker='.')\n","plt.scatter(df_validation.x,result,label='predicted',marker='.')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","# RMSE Metrics\n","print('XGBoost Algorithm Metrics')\n","mse = mean_squared_error(df_validation.y,result)\n","print(\" Mean Squared Error: {0:.2f}\".format(mse))\n","print(\" Root Mean Square Error: {0:.2f}\".format(mse**.5))\n","\n","# Residual\n","# Over prediction and Under Prediction needs to be balanced\n","# Training Data Residuals\n","residuals = df_validation.y - result\n","plt.hist(residuals)\n","plt.grid(True)\n","plt.xlabel('Actual - Predicted')\n","plt.ylabel('Count')\n","plt.title('XGBoost Residual')\n","plt.axvline(color='r')\n","plt.show()\n","\n","# Count number of values greater than zero and less than zero\n","value_counts = (residuals > 0).value_counts(sort=False)\n","\n","print(' Under Estimation: {0}'.format(value_counts[True]))\n","print(' Over  Estimation: {0}'.format(value_counts[False]))\n","\n","# Plot for entire dataset\n","plt.plot(df.x,df.y,label='Target')\n","plt.plot(df.x,regressor.predict(df[['x']]) ,label='Predicted')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('XGBoost')\n","plt.show()\n","\n","## Linear Regression Algorithm\n","\n","lin_regressor = LinearRegression()\n","\n","lin_regressor.fit(X_train,y_train)\n","\n","Compare Weights assigned by Linear Regression.\n","\n","Original Function: 5*x**2 -23*x + 47 + some noise\n","\n","Linear Regression Function: -15.08 * x + 709.86\n","\n","Linear Regression Coefficients and Intercepts are not close to actual\n","\n","lin_regressor.coef_\n","\n","lin_regressor.intercept_\n","\n","result = lin_regressor.predict(df_validation[['x']])\n","\n","plt.title('LinearRegression - Validation Dataset')\n","plt.scatter(df_validation.x,df_validation.y,label='actual',marker='.')\n","plt.scatter(df_validation.x,result,label='predicted',marker='.')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","# RMSE Metrics\n","print('Linear Regression Metrics')\n","mse = mean_squared_error(df_validation.y,result)\n","print(\" Mean Squared Error: {0:.2f}\".format(mse))\n","print(\" Root Mean Square Error: {0:.2f}\".format(mse**.5))\n","\n","# Residual\n","# Over prediction and Under Prediction needs to be balanced\n","# Training Data Residuals\n","residuals = df_validation.y - result\n","plt.hist(residuals)\n","plt.grid(True)\n","plt.xlabel('Actual - Predicted')\n","plt.ylabel('Count')\n","plt.title('Linear Regression Residual')\n","plt.axvline(color='r')\n","plt.show()\n","\n","# Count number of values greater than zero and less than zero\n","value_counts = (residuals > 0).value_counts(sort=False)\n","\n","print(' Under Estimation: {0}'.format(value_counts[True]))\n","print(' Over  Estimation: {0}'.format(value_counts[False]))\n","\n","# Plot for entire dataset\n","plt.plot(df.x,df.y,label='Target')\n","plt.plot(df.x,lin_regressor.predict(df[['x']]) ,label='Predicted')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('LinearRegression')\n","plt.show()\n","\n","Linear Regression is showing clear symptoms of under-fitting\n","\n","Input Features are not sufficient to capture complex relationship\n","\n","<h2>Your Turn</h2>\n","You can correct this under-fitting issue by adding relavant features.\n","\n","1. What feature will you add and why?\n","2. Complete the code and Test\n","3. What performance do you see now?\n","\n","# Specify the column names as the file does not have column header\n","df_train = pd.read_csv(train_file,names=['y','x'])\n","df_validation = pd.read_csv(validation_file,names=['y','x'])\n","df = pd.read_csv('quadratic_all.csv')\n","\n","# Add new features\n","\n","# Place holder to add new features to df_train, df_validation and df\n","# if you need help, scroll down to see the answer\n","# Add your code\n","\n","X_train = df_train.iloc[:,1:] # Features: 1st column onwards\n","y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n","\n","X_validation = df_validation.iloc[:,1:]\n","y_validation = df_validation.iloc[:,0].ravel()\n","\n","lin_regressor.fit(X_train,y_train)\n","\n","Original Function: -23*x + 5*x**2 + 47 + some noise (rewritten with x term first)\n","\n","lin_regressor.coef_\n","\n","lin_regressor.intercept_\n","\n","result = lin_regressor.predict(X_validation)\n","\n","plt.title('LinearRegression - Validation Dataset')\n","plt.scatter(df_validation.x,df_validation.y,label='actual',marker='.')\n","plt.scatter(df_validation.x,result,label='predicted',marker='.')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","# RMSE Metrics\n","print('Linear Regression Metrics')\n","mse = mean_squared_error(df_validation.y,result)\n","print(\" Mean Squared Error: {0:.2f}\".format(mse))\n","print(\" Root Mean Square Error: {0:.2f}\".format(mse**.5))\n","\n","print(\"***You should see an RMSE score of 30.45 or less\")\n","\n","df.head()\n","\n","# Plot for entire dataset\n","plt.plot(df.x,df.y,label='Target')\n","plt.plot(df.x,lin_regressor.predict(df[['x','x2']]) ,label='Predicted')\n","plt.grid(True)\n","plt.xlabel('Input Feature')\n","plt.ylabel('Target')\n","plt.legend()\n","plt.title('LinearRegression')\n","plt.show()\n","\n","## Solution for under-fitting\n","\n","add a new X**2 term to the dataframe\n","\n","syntax:\n","\n","df_train['x2'] = df_train['x']**2\n","\n","df_validation['x2'] = df_validation['x']**2\n","\n","df['x2'] = df['x']**2\n","\n","### Tree Based Algorithms have a lower bound and upper bound for predicted values\n","\n","# True Function\n","def quad_func (x):\n","    return 5*x**2 -23*x + 47\n","\n","# X is outside range of training samples\n","# New Feature: Adding X^2 term\n","\n","X = np.array([-100,-25,25,1000,5000])\n","y = quad_func(X)\n","df_tmp = pd.DataFrame({'x':X,'y':y,'x2':X**2})\n","df_tmp['xgboost']=regressor.predict(df_tmp[['x']])\n","df_tmp['linear']=lin_regressor.predict(df_tmp[['x','x2']])\n","\n","df_tmp\n","\n","plt.scatter(df_tmp.x,df_tmp.y,label='Actual',color='r')\n","plt.plot(df_tmp.x,df_tmp.linear,label='LinearRegression')\n","plt.plot(df_tmp.x,df_tmp.xgboost,label='XGBoost')\n","plt.legend()\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.title('Input Outside Range')\n","plt.show()\n","\n","# X is inside range of training samples\n","X = np.array([-15,-12,-5,0,1,3,5,7,9,11,15,18])\n","y = quad_func(X)\n","df_tmp = pd.DataFrame({'x':X,'y':y,'x2':X**2})\n","df_tmp['xgboost']=regressor.predict(df_tmp[['x']])\n","df_tmp['linear']=lin_regressor.predict(df_tmp[['x','x2']])\n","\n","df_tmp\n","\n","# XGBoost Predictions have an upper bound and lower bound\n","# Linear Regression Extrapolates\n","plt.scatter(df_tmp.x,df_tmp.y,label='Actual',color='r')\n","plt.plot(df_tmp.x,df_tmp.linear,label='LinearRegression')\n","plt.plot(df_tmp.x,df_tmp.xgboost,label='XGBoost')\n","plt.legend()\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.title('Input within range')\n","plt.show()\n","\n","<h2>Summary</h2>\n","\n","1. In this exercise, we compared performance of XGBoost model and Linear Regression on a quadratic dataset\n","2. The relationship between input feature and target was non-linear.\n","3. XGBoost handled it pretty well; whereas, linear regression was under-fitting\n","4. To correct the issue, we had to add additional features for linear regression\n","5. With this change, linear regression performed much better\n","\n","XGBoost can detect patterns involving non-linear relationship; whereas, algorithms like linear regression may need complex feature engineering\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"OfIdbqY9Vp00"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Bike** **Code**"],"metadata":{"id":"9hkXxQBFXrpu"}},{"cell_type":"code","source":["#Name : bikerental_data_preparation_rev1\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from pandas.plotting import register_matplotlib_converters\n","register_matplotlib_converters()\n","# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.register_matplotlib_converters.html\n","# Register converters for handling timestamp values in plots\n","\n","<h2>Kaggle Bike Sharing Demand Dataset</h2>\n","<h4>To download dataset, sign-in and download from this link: https://www.kaggle.com/c/bike-sharing-demand/data</h4>\n","<br>\n","\n","Input Features:<br>\n","['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'dayofweek','hour']<br>\n","\n","Target:<br>\n","['count']<br>\n","\n","Objective:\n","\n","You are provided hourly rental data spanning two years.\n","\n","For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month.\n","\n","You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period\n","\n","Reference: https://www.kaggle.com/c/bike-sharing-demand/data\n","\n","columns = ['count', 'season', 'holiday', 'workingday', 'weather', 'temp',\n","       'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'dayofweek','hour']\n","\n","df = pd.read_csv('train.csv', parse_dates=['datetime'],index_col=0)\n","df_test = pd.read_csv('test.csv', parse_dates=['datetime'],index_col=0)\n","\n","df.head()\n","\n","# We need to convert datetime to numeric for training.\n","# Let's extract key features into separate numeric columns\n","def add_features(df):\n","    df['year'] = df.index.year\n","    df['month'] = df.index.month\n","    df['day'] = df.index.day\n","    df['dayofweek'] = df.index.dayofweek\n","    df['hour'] = df.index.hour\n","\n","# Add New Features\n","add_features(df)\n","add_features(df_test)\n","\n","df.head()\n","\n","# Need to predict the missing data\n","plt.title('Rental Count - Gaps')\n","df['2011-01':'2011-02']['count'].plot()\n","plt.show()\n","\n","# Rentals change hourly!\n","plt.plot(df['2011-01-01']['count'])\n","plt.xticks(fontsize=14, rotation=45)\n","plt.xlabel('Date')\n","plt.ylabel('Rental Count')\n","plt.title('Hourly Rentals for Jan 01, 2011')\n","plt.show()\n","\n","# Seasonal\n","plt.plot(df['2011-01']['count'])\n","plt.xticks(fontsize=14, rotation=45)\n","plt.xlabel('Date')\n","plt.ylabel('Rental Count')\n","plt.title('Jan 2011 Rentals (1 month)')\n","plt.show()\n","\n","group_hour = df.groupby(['hour'])\n","average_by_hour = group_hour['count'].mean()\n","\n","plt.plot(average_by_hour.index,average_by_hour)\n","plt.xlabel('Hour')\n","plt.ylabel('Rental Count')\n","plt.xticks(np.arange(24))\n","plt.grid(True)\n","plt.title('Average Hourly Rental Count')\n","\n","# Year to year trend\n","plt.plot(df['2011']['count'],label='2011')\n","plt.plot(df['2012']['count'],label='2012')\n","plt.xticks(fontsize=14, rotation=45)\n","plt.xlabel('Date')\n","plt.ylabel('Rental Count')\n","plt.title('2011 and 2012 Rentals (Year to Year)')\n","plt.legend()\n","plt.show()\n","\n","group_year_month = df.groupby(['year','month'])\n","\n","average_year_month = group_year_month['count'].mean()\n","\n","average_year_month\n","\n","for year in average_year_month.index.levels[0]:\n","    plt.plot(average_year_month[year].index,average_year_month[year],label=year)\n","\n","plt.legend()\n","plt.xlabel('Month')\n","plt.ylabel('Count')\n","plt.grid(True)\n","plt.title('Average Monthly Rental Count for 2011, 2012')\n","plt.show()\n","\n","group_year_hour = df.groupby(['year','hour'])\n","average_year_hour = group_year_hour['count'].mean()\n","for year in average_year_hour.index.levels[0]:\n","    #print (year)\n","    #print(average_year_month[year])\n","    plt.plot(average_year_hour[year].index,average_year_hour[year],label=year)\n","\n","plt.legend()\n","plt.xlabel('Hour')\n","plt.ylabel('Count')\n","plt.xticks(np.arange(24))\n","plt.grid(True)\n","plt.title('Average Hourly Rental Count - 2011, 2012')\n","\n","group_workingday_hour = df.groupby(['workingday','hour'])\n","average_workingday_hour = group_workingday_hour['count'].mean()\n","\n","for workingday in average_workingday_hour.index.levels[0]:\n","    #print (year)\n","    #print(average_year_month[year])\n","    plt.plot(average_workingday_hour[workingday].index,average_workingday_hour[workingday],\n","             label=workingday)\n","\n","plt.legend()\n","plt.xlabel('Hour')\n","plt.ylabel('Count')\n","plt.xticks(np.arange(24))\n","plt.grid(True)\n","plt.title('Average Hourly Rental Count by Working Day')\n","plt.show()\n","\n","# Let's look at correlation beween features and target\n","df.corr()['count']\n","\n","# Any relation between temperature and rental count?\n","plt.scatter(x=df.temp,y=df[\"count\"])\n","plt.grid(True)\n","plt.xlabel('Temperature')\n","plt.ylabel('Count')\n","plt.title('Temperature vs Count')\n","plt.show()\n","\n","# Any relation between humidity and rental count?\n","plt.scatter(x=df.humidity,y=df[\"count\"],label='Humidity')\n","plt.grid(True)\n","plt.xlabel('Humidity')\n","plt.ylabel('Count')\n","plt.title('Humidity vs Count')\n","plt.show()\n","\n","# Save all data\n","df.to_csv('bike_all.csv',index=True,index_label='datetime',columns=columns)\n","\n","## Training and Validation Set\n","### Target Variable as first column followed by input features\n","### Training, Validation files do not have a column header\n","\n","# Training = 70% of the data\n","# Validation = 30% of the data\n","# Randomize the datset\n","np.random.seed(5)\n","l = list(df.index)\n","np.random.shuffle(l)\n","df = df.loc[l]\n","\n","rows = df.shape[0]\n","train = int(.7 * rows)\n","test = rows-train\n","\n","rows, train, test\n","\n","columns\n","\n","# Write Training Set\n","df.iloc[:train].to_csv('bike_train.csv'\n","                          ,index=False,header=False\n","                          ,columns=columns)\n","\n","# Write Validation Set\n","df.iloc[train:].to_csv('bike_validation.csv'\n","                          ,index=False,header=False\n","                          ,columns=columns)\n","\n","# Test Data has only input features\n","df_test.to_csv('bike_test.csv',index=True,index_label='datetime')\n","\n","print(','.join(columns))\n","\n","\n","\n","# Write Column List\n","with open('bike_train_column_list.txt','w') as f:\n","    f.write(','.join(columns))\n","\n","## Train a model with bike rental data using XGBoost algorithm\n","###  Model is trained with XGBoost installed in notebook instance\n","###  In the later examples, we will train using SageMaker's XGBoost algorithm\n","\n","\n","\n","\n","\n","#NAME : bikerental_xgboost_localmode_rev1\n","# Install xgboost in notebook instance.\n","#### Command to install xgboost\n","!pip install xgboost==1.2\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","# XGBoost\n","import xgboost as xgb\n","\n","column_list_file = 'bike_train_column_list.txt'\n","train_file = 'bike_train.csv'\n","validation_file = 'bike_validation.csv'\n","test_file = 'bike_test.csv'\n","\n","columns = ''\n","with open(column_list_file,'r') as f:\n","    columns = f.read().split(',')\n","\n","columns\n","\n","# Specify the column names as the file does not have column header\n","df_train = pd.read_csv(train_file,names=columns)\n","df_validation = pd.read_csv(validation_file,names=columns)\n","\n","df_train.head()\n","\n","df_validation.head()\n","\n","X_train = df_train.iloc[:,1:] # Features: 1st column onwards\n","y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n","\n","X_validation = df_validation.iloc[:,1:]\n","y_validation = df_validation.iloc[:,0].ravel()\n","\n","# XGBoost Training Parameter Reference:\n","#   https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n","#regressor = xgb.XGBRegressor(max_depth=5,eta=0.1,subsample=0.7,num_round=150)\n","regressor = xgb.XGBRegressor(max_depth=5,n_estimators=150)\n","\n","regressor\n","\n","regressor.fit(X_train,y_train, eval_set = [(X_train, y_train), (X_validation, y_validation)])\n","\n","eval_result = regressor.evals_result()\n","\n","training_rounds = range(len(eval_result['validation_0']['rmse']))\n","\n","print(training_rounds)\n","\n","plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n","plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n","plt.grid(True)\n","plt.xlabel('Iteration')\n","plt.ylabel('RMSE')\n","plt.title('Training Vs Validation Error')\n","plt.legend()\n","plt.show()\n","\n","xgb.plot_importance(regressor)\n","plt.show()\n","\n","# Verify Quality using Validation dataset\n","# Compare actual vs predicted performance with dataset not seen by the model before\n","df = pd.read_csv(validation_file,names=columns)\n","\n","df.head()\n","\n","df.shape\n","\n","X_test = df.iloc[:,1:]\n","print(X_test[:5])\n","\n","result = regressor.predict(X_test)\n","\n","result[:5]\n","\n","df['count_predicted'] = result\n","\n","df.head()\n","\n","# Negative Values are predicted\n","df['count_predicted'].describe()\n","\n","df[df['count_predicted'] < 0]\n","\n","df['count_predicted'].hist()\n","plt.title('Predicted Count Histogram')\n","plt.show()\n","\n","def adjust_count(x):\n","    if x < 0:\n","        return 0\n","    else:\n","        return x\n","\n","df['count_predicted'] = df['count_predicted'].map(adjust_count)\n","\n","df[df['count_predicted'] < 0]\n","\n","# Actual Vs Predicted\n","plt.plot(df['count'], label='Actual')\n","plt.plot(df['count_predicted'],label='Predicted')\n","plt.xlabel('Sample')\n","plt.ylabel('Count')\n","plt.xlim([100,150])\n","plt.title('Validation Dataset - Predicted Vs. Actual')\n","plt.legend()\n","plt.show()\n","\n","# Over prediction and Under Prediction needs to be balanced\n","# Training Data Residuals\n","residuals = (df['count'] - df['count_predicted'])\n","\n","plt.hist(residuals)\n","plt.grid(True)\n","plt.xlabel('Actual - Predicted')\n","plt.ylabel('Count')\n","plt.title('Residuals Distribution')\n","plt.axvline(color='r')\n","plt.show()\n","\n","value_counts = (residuals > 0).value_counts(sort=False)\n","print(' Under Estimation: {0:0.2f}'.format(value_counts[True]/len(residuals)))\n","print(' Over  Estimation: {0:0.2f}'.format(value_counts[False]/len(residuals)))\n","\n","print(\"RMSE: {0:0.2f}\".format(mean_squared_error(df['count'],df['count_predicted'])**.5))\n","\n","# RMSlE - Root Mean Squared Log Error\n","# RMSLE Metric is used by Kaggle for this competition\n","\n","# RMSE Cost Function - Magnitude of difference matters\n","\n","# RMSLE cost function - \"Only Percentage difference matters\"\n","\n","# Reference:Katerina Malahova, Khor SoonHin\n","# https://www.slideshare.net/KhorSoonHin/rmsle-cost-function\n","def compute_rmsle(y_true, y_pred):\n","    if type(y_true) != np.ndarray:\n","        y_true = np.array(y_true)\n","\n","    if type(y_pred) != np.ndarray:\n","        y_pred = np.array(y_pred)\n","\n","    return(np.average((np.log1p(y_pred) - np.log1p(y_true))**2)**.5)\n","\n","print('RMSLE')\n","print(compute_rmsle(100,50),\n","      compute_rmsle(1000,500),\n","      compute_rmsle(10000,5000))\n","\n","print('RMSLE')\n","print(compute_rmsle(100,25),\n","      compute_rmsle(1000,250),\n","      compute_rmsle(10000,2500))\n","\n","print('RMSE')\n","print(mean_squared_error([100],[50])**.5,\n","      mean_squared_error([1000],[500])**.5,\n","      mean_squared_error([10000],[5000])**.5)\n","\n","print('RMSE')\n","print(mean_squared_error([100],[25])**.5,\n","      mean_squared_error([1000],[250])**.5,\n","      mean_squared_error([10000],[2500])**.5)\n","\n","print(\"RMSLE: {0}\".format(compute_rmsle(df['count'],df['count_predicted'])))\n","\n","# Prepare Data for Submission to Kaggle\n","df_test = pd.read_csv(test_file,parse_dates=['datetime'])\n","\n","df_test.head()\n","\n","X_test =  df_test.iloc[:,1:] # Exclude datetime for prediction\n","\n","X_test.head()\n","\n","result = regressor.predict(X_test)\n","\n","result[:5]\n","\n","df_test[\"count\"] = result\n","\n","df_test.head()\n","\n","df_test[df_test[\"count\"] < 0]\n","\n","df_test[\"count\"] = df_test[\"count\"].map(adjust_count)\n","\n","df_test[['datetime','count']].to_csv('predicted_count.csv',index=False)\n","\n","# RMSLE (Kaggle) Score\n","# Test 1: 0.62\n","\n","\n","# Bike Train -- SageMaker\n","# XGBoost Built-in Algorithm - Bike Rental Regression Example\n","\n","import numpy as np\n","import pandas as pd\n","\n","import boto3\n","import re\n","\n","import sagemaker\n","from sagemaker import get_execution_role\n","# SageMaker SDK Documentation: http://sagemaker.readthedocs.io/en/latest/estimators.html\n","\n","## Upload Data to S3\n","\n","# Specify your bucket name\n","bucket_name = 'kayode-ml-sagemaker'\n","\n","training_folder = r'bikerental/training/'\n","validation_folder = r'bikerental/validation/'\n","test_folder = r'bikerental/test/'\n","\n","s3_model_output_location = r's3://{0}/bikerental/model'.format(bucket_name)\n","s3_training_file_location = r's3://{0}/{1}'.format(bucket_name,training_folder)\n","s3_validation_file_location = r's3://{0}/{1}'.format(bucket_name,validation_folder)\n","s3_test_file_location = r's3://{0}/{1}'.format(bucket_name,test_folder)\n","\n","print(s3_model_output_location)\n","print(s3_training_file_location)\n","print(s3_validation_file_location)\n","print(s3_test_file_location)\n","\n","# Write and Reading from S3 is just as easy\n","# files are referred as objects in S3.\n","# file name is referred as key name in S3\n","\n","# File stored in S3 is automatically replicated across 3 different availability zones\n","# in the region where the bucket was created.\n","\n","# http://boto3.readthedocs.io/en/latest/guide/s3.html\n","def write_to_s3(filename, bucket, key):\n","    with open(filename,'rb') as f: # Read in binary mode\n","        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)\n","\n","write_to_s3('bike_train.csv',\n","            bucket_name,\n","            training_folder + 'bike_train.csv')\n","\n","write_to_s3('bike_validation.csv',\n","            bucket_name,\n","            validation_folder + 'bike_validation.csv')\n","\n","write_to_s3('bike_test.csv',\n","            bucket_name,\n","            test_folder + 'bike_test.csv')\n","\n","## Training Algorithm Docker Image\n","### SageMaker maintains a separate image for algorithm and region\n","https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n","\n","# Use Spot Instance - Save up to 90% of training cost by using spot instances when compared to on-demand instances\n","# Reference: https://github.com/aws-samples/amazon-sagemaker-managed-spot-training/blob/main/xgboost_built_in_managed_spot_training_checkpointing/xgboost_built_in_managed_spot_training_checkpointing.ipynb\n","\n","# if you are still on two-month free-tier you can use the on-demand instance by setting:\n","#   use_spot_instances = False\n","\n","# We will use spot for training\n","use_spot_instances = True\n","max_run = 3600 # in seconds\n","max_wait = 7200 if use_spot_instances else None # in seconds\n","\n","job_name = 'xgboost-bikerental-v1'\n","\n","checkpoint_s3_uri = None\n","\n","if use_spot_instances:\n","    checkpoint_s3_uri = f's3://{bucket_name}/bikerental/checkpoints/{job_name}'\n","\n","print (f'Checkpoint uri: {checkpoint_s3_uri}')\n","\n","# Establish a session with AWS\n","sess = sagemaker.Session()\n","\n","role = get_execution_role()\n","\n","# This role contains the permissions needed to train, deploy models\n","# SageMaker Service is trusted to assume this role\n","print(role)\n","\n","# https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html#sagemaker.image_uris.retrieve\n","\n","# SDK 2 uses image_uris.retrieve the container image location\n","\n","# Use XGBoost 1.2 version\n","container = sagemaker.image_uris.retrieve(\"xgboost\",sess.boto_region_name,version=\"1.2-2\")\n","\n","print (f'Using XGBoost Container {container}')\n","\n","## Build Model\n","\n","# Configure the training job\n","# Specify type and number of instances to use\n","# S3 location where final artifacts needs to be stored\n","\n","#   Reference: http://sagemaker.readthedocs.io/en/latest/estimators.html\n","\n","# for managed spot training, specify the use_spot_instances flag, max_run, max_wait and checkpoint_s3_uri\n","\n","# SDK 2.x version does not require train prefix for instance count and type\n","estimator = sagemaker.estimator.Estimator(\n","    container,\n","    role,\n","    instance_count=1,\n","    instance_type='ml.m5.xlarge',\n","    output_path=s3_model_output_location,\n","    sagemaker_session=sess,\n","    base_job_name = job_name,\n","    use_spot_instances=use_spot_instances,\n","    max_run=max_run,\n","    max_wait=max_wait,\n","    checkpoint_s3_uri=checkpoint_s3_uri)\n","\n","# Specify hyper parameters that appropriate for the training algorithm\n","# XGBoost Training Parameter Reference\n","#  https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters\n","estimator.set_hyperparameters(max_depth=5,\n","                              objective=\"reg:squarederror\",\n","                              eta=0.1,\n","                              num_round=150)\n","\n","estimator.hyperparameters()\n","\n","### Specify Training Data Location and Optionally, Validation Data Location\n","\n","# content type can be libsvm or csv for XGBoost\n","training_input_config = sagemaker.session.TrainingInput(\n","    s3_data=s3_training_file_location,\n","    content_type='csv',\n","    s3_data_type='S3Prefix')\n","\n","validation_input_config = sagemaker.session.TrainingInput(\n","    s3_data=s3_validation_file_location,\n","    content_type='csv',\n","    s3_data_type='S3Prefix'\n",")\n","\n","data_channels = {'train': training_input_config, 'validation': validation_input_config}\n","\n","print(training_input_config.config)\n","print(validation_input_config.config)\n","\n","### Train the model\n","\n","# XGBoost supports \"train\", \"validation\" channels\n","# Reference: Supported channels by algorithm\n","#   https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n","estimator.fit(data_channels)\n","\n","## Deploy Model\n","\n","# Ref: http://sagemaker.readthedocs.io/en/latest/estimators.html\n","predictor = estimator.deploy(initial_instance_count=1,\n","                             instance_type='ml.m5.xlarge',\n","                             endpoint_name = job_name)\n","\n","## Run Predictions\n","\n","# SDK 2.0 serializers\n","from sagemaker.serializers import CSVSerializer\n","\n","predictor.serializer = CSVSerializer()\n","\n","predictor.predict([[3,0,1,2,28.7,33.335,79,12.998,2011,7,7,3]])\n","\n","## Summary\n","\n","1. Ensure Training, Test and Validation data are in S3 Bucket\n","2. Select Algorithm Container Registry Path - Path varies by region\n","3. Configure Estimator for training - Specify Algorithm container, instance count, instance type, model output location\n","4. Specify algorithm specific hyper parameters\n","5. Train model\n","6. Deploy model - Specify instance count, instance type and endpoint name\n","7. Run Predictions\n","\n","# Name: Invoke Endpoint\n","#<h1>XGBoost Cloud Prediction Invocation Template</h1>\n","<h4>Invoke SageMaker Prediction Service</h4>\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","import os\n","\n","import boto3\n","import re # python regex module\n","from sagemaker import get_execution_role\n","import sagemaker\n","\n","# SDK 2 serializers and deserializers\n","from sagemaker.serializers import CSVSerializer\n","from sagemaker.deserializers import JSONDeserializer\n","\n","# SDK 2\n","# RealTimePredictor renamed to Predictor\n","# https://sagemaker.readthedocs.io/en/stable/v2.html\n","\n","# Create a predictor and point to an existing endpoint\n","endpoint_name = 'xgboost-bikerental-v1'\n","predictor = sagemaker.predictor.Predictor (endpoint_name=endpoint_name)\n","\n","predictor.serializer = CSVSerializer()\n","\n","df_all = pd.read_csv('bike_test.csv')\n","\n","df_all.head()\n","\n","df_all.columns[1:]\n","\n","# Need to pass an array to the prediction\n","# can pass a numpy array or a list of values [[19,1],[20,1]]\n","arr_test = df_all[df_all.columns[1:]].values\n","\n","type(arr_test)\n","\n","arr_test.shape\n","\n","arr_test[:5]\n","\n","result = predictor.predict(arr_test[:2])\n","\n","result\n","\n","arr_test.shape\n","\n","### Split the input data into chunks\n","There are thousands of rows in this data set for which need inference.\n","When communicating over internet, it is a good idea to split the data into chunks to prevent payload and timeout error\n","\n","# For large number of predictions, we can split the input data and\n","# Query the prediction service.\n","# array_split is convenient to specify how many splits are needed\n","\n","# Splitting using regular expression as xgboost 1-2-2 is returning\n","# predicted values with inconsistent delimiters (comma, newline or both)\n","\n","# pattern looks for one or more of non-numeric characters\n","pattern = r'[^0-9.]+'\n","\n","predictions = []\n","for arr in np.array_split(arr_test,10):\n","    result = predictor.predict(arr)\n","    result = re.split(pattern,result.decode(\"utf-8\"))\n","\n","    print (arr.shape)\n","    predictions += [float(r) for r in result if r != \"\"] # Thanks, Ionut Barbu!\n","\n","len(predictions)\n","\n","np.expm1(predictions)\n","\n","df_all['count'] = np.expm1(predictions)\n","\n","df_all.head()\n","\n","df_all[['datetime','count']].to_csv('predicted_count_cloud.csv',index=False)\n","\n","# Delete Endpoint to prevent unnecessary charges\n","predictor.delete_endpoint()\n","\n","\n","\n","\n","# Multil class Classification\n","# iris_data_preparation\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","\n","<h2>Iris Classification Dataset</h2>\n","\n","Input Features:<br>\n","sepal_length,sepal_width,petal_length,petal_width<br>\n","\n","Target:<br>\n","Iris plant class<br>\n","\n","Objective: Predict iris plant class for a given sepal_length,sepal_width,petal_length,petal_width<br>\n","<h4>Data source: https://archive.ics.uci.edu/ml/datasets/iris</h4>\n","\n","columns = ['encoded_class','sepal_length','sepal_width','petal_length','petal_width']\n","\n","# Encode Class Labels to integers\n","le = preprocessing.LabelEncoder()\n","le.fit(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n","\n","le.classes_\n","\n","df = pd.read_csv('iris_all.csv')\n","\n","df['class'].value_counts()\n","\n","df.head()\n","\n","df.tail()\n","\n","le.transform(df['class'])[-5:]\n","\n","# Convert Classes to numeric value\n","df['encoded_class'] = le.transform(df['class'])\n","\n","df.head()\n","\n","df.tail()\n","\n","# Visualize\n","setosa = df['class'] == 'Iris-setosa'\n","versicolor = df['class'] == 'Iris-versicolor'\n","virginica = df['class'] == 'Iris-virginica'\n","\n","plt.scatter(df[setosa].sepal_length,y=df[setosa].sepal_width, label='setosa',color='g')\n","plt.scatter(df[versicolor].sepal_length,y=df[versicolor].sepal_width, label='versicolor',color='r')\n","plt.scatter(df[virginica].sepal_length,y=df[virginica].sepal_width, label='virginica',color='b')\n","plt.xlabel('length')\n","plt.ylabel('width')\n","plt.title('Sepal')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","plt.scatter(df[setosa].petal_length,y=df[setosa].petal_width, label='setosa',color='g')\n","plt.scatter(df[versicolor].petal_length,y=df[versicolor].petal_width, label='versicolor',color='r')\n","plt.scatter(df[virginica].petal_length,y=df[virginica].petal_width, label='virginica',color='b')\n","plt.xlabel('length')\n","plt.ylabel('width')\n","plt.title('Petal')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","plt.scatter(df[setosa].petal_length,y=df[setosa].sepal_length, label='setosa',color='g')\n","plt.scatter(df[versicolor].petal_length,y=df[versicolor].sepal_length, label='versicolor',color='r')\n","plt.scatter(df[virginica].petal_length,y=df[virginica].sepal_length, label='virginica',color='b')\n","plt.xlabel('petal length')\n","plt.ylabel('sepal length')\n","plt.title('Petal-Sepal')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","## Training and Validation Set\n","### Target Variable as first column followed by input features:\n","class,sepal_length,sepal_width,petal_length,petal_width\n","### Training, Validation files do not have a column header\n","\n","# Training = 70% of the data\n","# Validation = 30% of the data\n","# Randomize the datset\n","np.random.seed(5)\n","l = list(df.index)\n","np.random.shuffle(l)\n","df = df.iloc[l]\n","\n","rows = df.shape[0]\n","train = int(.7 * rows)\n","test = rows-train\n","\n","rows, train, test\n","\n","# Write Training Set\n","df[:train].to_csv('iris_train.csv'\n","                          ,index=False,header=False\n","                          ,columns=columns)\n","\n","# Write Validation Set\n","df[train:].to_csv('iris_validation.csv'\n","                          ,index=False,header=False\n","                          ,columns=columns)\n","\n","# Write Column List\n","with open('iris_train_column_list.txt','w') as f:\n","    f.write(','.join(columns))\n","\n","\n","# Iris Multiclass Classification\n","## Train a model with Iris data using XGBoost algorithm\n","###  Model is trained with XGBoost installed in notebook instance\n","###  In the later examples, we will train using SageMaker's XGBoost algorithm\n","\n","# Install xgboost in notebook instance.\n","#### Command to install xgboost\n","!pip install xgboost==1.2\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import itertools\n","import xgboost as xgb\n","\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","column_list_file = 'iris_train_column_list.txt'\n","train_file = 'iris_train.csv'\n","validation_file = 'iris_validation.csv'\n","\n","columns = ''\n","with open(column_list_file,'r') as f:\n","    columns = f.read().split(',')\n","\n","columns\n","\n","# Encode Class Labels to integers\n","# Labeled Classes\n","labels=[0,1,2]\n","classes = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n","le = preprocessing.LabelEncoder()\n","le.fit(classes)\n","\n","# Specify the column names as the file does not have column header\n","df_train = pd.read_csv(train_file,names=columns)\n","df_validation = pd.read_csv(validation_file,names=columns)\n","\n","df_train.head()\n","\n","df_validation.head()\n","\n","X_train = df_train.iloc[:,1:] # Features: 1st column onwards\n","y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n","\n","X_validation = df_validation.iloc[:,1:]\n","y_validation = df_validation.iloc[:,0].ravel()\n","\n","# Launch a classifier\n","# XGBoost Training Parameter Reference:\n","#   https://xgboost.readthedocs.io/en/latest/parameter.html\n","\n","classifier = xgb.XGBClassifier(objective=\"multi:softmax\",\n","                               num_class=3,\n","                               n_estimators=100)\n","\n","classifier\n","\n","classifier.fit(X_train,\n","               y_train,\n","               eval_set = [(X_train, y_train), (X_validation, y_validation)],\n","               eval_metric=['mlogloss'],\n","               early_stopping_rounds=10)\n","\n","# early_stopping_rounds - needs to be passed in as a hyperparameter in SageMaker XGBoost implementation\n","# \"The model trains until the validation score stops improving.\n","# Validation error needs to decrease at least every early_stopping_rounds to continue training.\n","# Amazon SageMaker hosting uses the best model for inference.\"\n","\n","eval_result = classifier.evals_result()\n","\n","training_rounds = range(len(eval_result['validation_0']['mlogloss']))\n","\n","print(training_rounds)\n","\n","plt.scatter(x=training_rounds,y=eval_result['validation_0']['mlogloss'],label='Training Error')\n","plt.scatter(x=training_rounds,y=eval_result['validation_1']['mlogloss'],label='Validation Error')\n","plt.grid(True)\n","plt.xlabel('Iteration')\n","plt.ylabel('LogLoss')\n","plt.title('Training Vs Validation Error')\n","plt.legend()\n","plt.show()\n","\n","xgb.plot_importance(classifier)\n","plt.show()\n","\n","df = pd.read_csv(validation_file,names=columns)\n","\n","df.head()\n","\n","X_test = df.iloc[:,1:]\n","print(X_test[:5])\n","\n","result = classifier.predict(X_test)\n","\n","result[:5]\n","\n","df['predicted_class'] = result #le.inverse_transform(result)\n","\n","df.head()\n","\n","# Compare performance of Actual and Model 1 Prediction\n","plt.figure()\n","plt.scatter(df.index,df['encoded_class'],label='Actual')\n","plt.scatter(df.index,df['predicted_class'],label='Predicted',marker='^')\n","plt.legend(loc=4)\n","plt.yticks([0,1,2])\n","plt.xlabel('Sample')\n","plt.ylabel('Class')\n","plt.show()\n","\n","<h2>Confusion Matrix</h2>\n","Confusion Matrix is a table that summarizes performance of classification model.<br><br>\n","\n","# Reference:\n","# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        #print(\"Normalized confusion matrix\")\n","    #else:\n","    #    print('Confusion matrix, without normalization')\n","\n","    #print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()\n","\n","# Compute confusion matrix\n","cnf_matrix = confusion_matrix(df['encoded_class'],\n","                              df['predicted_class'],labels=labels)\n","\n","cnf_matrix\n","\n","# Plot confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=classes,\n","                      title='Confusion matrix - Count')\n","\n","# Plot confusion matrix\n","plt.figure()\n","plot_confusion_matrix(cnf_matrix, classes=classes,\n","                      title='Confusion matrix - Count',normalize=True)\n","\n","print(classification_report(\n","    df['encoded_class'],\n","    df['predicted_class'],\n","    labels=labels,\n","    target_names=classes))\n","\n","\n","## iNVOKE Endpoint\n","## Invoke SageMaker Enpoint from outside of AWS environment using SageMaker SDK\n","\n","Model used: XGBoost Bike Rental Prediction Trained in the XGBoost Lectures\n","\n","This example uses the IAM user: ml_user_predict. The user was setup in the housekeeping lecture of the course.\n","\n","Refer to the lecture: Configure IAM Users, Setup Command Line Interface (CLI)\n","\n","Ensure xgboost-biketrain-v1 Endpoint is deployed before running this example\n","\n","To create an endpoint using SageMaker Console:\n","1. Select \"Models\" under \"Inference\" in navigation pane\n","2. Search for model using this prefix: xgboost-biketrain-v1\n","3. Select the latest model and choose create endpoint\n","4. Specify endpoint name as: xgboost-biketrain-v1\n","5. Create a new endpoint configuration\n","6. Create a new endpoint\n","7. After this lab is completed, delete the endpoint to avoid unnecessary charges\n","\n","# Install SageMaker 2.x version.\n","!pip install --upgrade sagemaker\n","\n","import boto3\n","import sagemaker\n","import math\n","import dateutil\n","import re\n","\n","# SDK 2 serializers and deserializers\n","from sagemaker.serializers import CSVSerializer\n","from sagemaker.deserializers import JSONDeserializer\n","\n","# Establish a session with AWS\n","# Specify credentials and region to be used for this session.\n","# We will use a ml_user_predict credentials that has limited privileges\n","boto_session = boto3.Session(profile_name='ml_user_predict',region_name='us-east-1')\n","\n","sess = sagemaker.Session(boto_session=boto_session)\n","\n","# Create a predictor and point to an existing endpoint\n","\n","# Get Predictor using SageMaker SDK\n","# Specify Your Endpoint Name\n","endpoint_name = 'xgboost-biketrain-v1'\n","\n","predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name,\n","                                                 sagemaker_session=sess)\n","\n","# We are sending data for inference in CSV format\n","predictor.serializer = CSVSerializer()\n","\n","#datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed\n","# Actual=562\n","sample_one = '2012-12-19 17:00:00,4,0,1,1,16.4,20.455,50,26.0027'\n","# Actual=569\n","sample_two = '2012-12-19 18:00:00,4,0,1,1,15.58,19.695,50,23.9994'\n","# Actual=4\n","sample_three = '2012-12-10 01:00:00,4,0,1,2,14.76,18.94,100,0'\n","\n","# Raw Data Structure:\n","# datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n","\n","# Model expects data in this format (it was trained with these features):\n","# season,holiday,workingday,weather,temp,atemp,humidity,windspeed,year,month,day,dayofweek,hour\n","\n","def transform_data(data):\n","    features = data.split(',')\n","\n","    # Extract year, month, day, dayofweek, hour\n","    dt = dateutil.parser.parse(features[0])\n","\n","    features.append(str(dt.year))\n","    features.append(str(dt.month))\n","    features.append(str(dt.day))\n","    features.append(str(dt.weekday()))\n","    features.append(str(dt.hour))\n","\n","    # Return the transformed data. skip datetime field\n","    return ','.join(features[1:])\n","\n","print('Raw Data:\\n',sample_one)\n","print('Transformed Data:\\n',transform_data(sample_one))\n","\n","# Let's invoke prediction now\n","predictor.predict(transform_data(sample_one))\n","\n","# Actual Count is 562...but predicted is 6.3.\n","\n","# Model was trained with log1p(count)\n","# So, we need to apply inverse transformation to get the actual count\n","# Predicted Count looks much better now\n","result = predictor.predict(transform_data(sample_one))\n","result = result.decode(\"utf-8\")\n","print ('Predicted Count', math.expm1(float(result)))\n","\n","# how to send multiple samples\n","result = predictor.predict([transform_data(sample_one), transform_data(sample_two)])\n","\n","result.decode(\"utf-8\")\n","\n","# Batch Prediction\n","# Transform data and invoke prediction in specified batch sizes\n","def run_predictions(data, batch_size):\n","    predictions = []\n","\n","    transformed_data = [transform_data(row.strip()) for row in data]\n","\n","    # Splitting using regular expression as xgboost 1-2-2 is returning\n","    # predicted values with inconsistent delimiters (comma, newline or both)\n","\n","    # pattern looks for one or more of non-numeric characters\n","    pattern = r'[^0-9.]+'\n","\n","    for i in range(0, len(data), batch_size):\n","\n","        print(i,i+batch_size)\n","\n","        result = predictor.predict(transformed_data[i : i + batch_size])\n","\n","        result = result.decode(\"utf-8\")\n","        result = re.split(pattern,result)\n","\n","        predictions += [math.expm1(float(r)) for r in result if r != \"\"]\n","\n","    return predictions\n","\n","run_predictions([sample_one,sample_two,sample_three],10)\n","\n","# Run a batch prediction on Test.CSV File\n","# Read the file content\n","data = []\n","with open('test.csv','r') as f:\n","    # skip header\n","    f.readline()\n","    # Read remaining lines\n","    data = f.readlines()\n","\n","len(data)\n","\n","%%time\n","predictions = run_predictions(data,500)\n","\n","len(predictions),len(data)\n","\n","# Don't forget to delete the endpoint\n","# From SageMaker Console, Select \"Endpoints\" under Inference and Delete the Endpoint\n","\n","\n","#Invoke with BOTO3 SDK\n","# Boto3 SageMaker Invoke Endpoint\n","# This example shows how to invoke SageMaker Endpoint from outside of AWS environment using Boto3 SDK\n","# Boto is the Amazon Web Services (AWS) SDK for Python\n","# https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n","\n","# Endpoint: XGBoost - Kaggle Bike Rental - Regressor Trained in XGBoost Lectures\n","# Makesure Endpoint is deployed before running this example\n","#\n","# Reference:\n","#  https://github.com/awslabs/amazon-sagemaker-examples\n","\n","# NOTE: SageMaker SDK now requires additional permissions DescribeEndpoint, DescribeEndpointConfig in-addition to InvokeEndpoint\n","#   boto3 SDK requires just InvokeEndpoint permission.\n","#   Please update SageMakerInvokeEndpoint permissions to reflect this policy document:\n","#   Logon with my_admin account and update permissions (IAM->Policies->SageMakerInvokeEndpoint->Edit Policy)\n","#\n","{\n","    \"Version\": \"2012-10-17\",\n","    \"Statement\": [\n","        {\n","            \"Sid\": \"VisualEditor0\",\n","            \"Effect\": \"Allow\",\n","            \"Action\": [\n","                \"sagemaker:DescribeEndpointConfig\",\n","                \"sagemaker:DescribeEndpoint\",\n","                \"sagemaker:InvokeEndpoint\"\n","            ],\n","            \"Resource\": \"*\"\n","        }\n","    ]\n","}\n","\n","import boto3\n","import math\n","import dateutil\n","import re\n","\n","# Establish a session with AWS\n","# Specify credentials and region to be used for this session.\n","# We will use a ml_user_predict credentials that has limited privileges\n","boto_session = boto3.Session(profile_name='ml_user_predict',region_name='us-east-1')\n","\n","# List of low level clients that are available in boto3\n","print(boto_session.get_available_services())\n","\n","# Acquire a SageMaker Runtime client for us-east-1 region\n","client = boto_session.client(service_name='sagemaker-runtime',region_name='us-east-1')\n","\n","# Specify Your Endpoint Name\n","endpoint_name = 'xgboost-biketrain-v1'\n","\n","# Raw Data\n","#datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n","# Actual=562\n","sample_one = '2012-12-19 17:00:00,4,0,1,1,16.4,20.455,50,26.0027'\n","# Actual=569\n","sample_two = '2012-12-19 18:00:00,4,0,1,1,15.58,19.695,50,23.9994'\n","# Actual=4\n","sample_three = '2012-12-10 01:00:00,4,0,1,2,14.76,18.94,100,0'\n","\n","# Raw Data Structure:\n","# datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n","\n","# Model expects data in this format (it was trained with these features):\n","# season,holiday,workingday,weather,temp,atemp,humidity,windspeed,year,month,day,dayofweek,hour\n","\n","def transform_data(data):\n","    features = data.split(',')\n","\n","    # Extract year, month, day, dayofweek, hour\n","    dt = dateutil.parser.parse(features[0])\n","\n","    features.append(str(dt.year))\n","    features.append(str(dt.month))\n","    features.append(str(dt.day))\n","    features.append(str(dt.weekday()))\n","    features.append(str(dt.hour))\n","\n","    # Return the transformed data. skip datetime field\n","    return ','.join(features[1:])\n","\n","print('Raw Data:\\n',sample_one)\n","print('Transformed Data:\\n',transform_data(sample_one))\n","\n","# Let's invoke prediction now\n","result = client.invoke_endpoint(EndpointName=endpoint_name,\n","                       Body=transform_data(sample_one).encode('utf-8'),\n","                       ContentType='text/csv')\n","\n","result = result['Body'].read().decode('utf-8')\n","\n","print(result)\n","\n","# Actual Count is 562...but predicted is 6.36.\n","\n","# Model was trained with log1p(count)\n","# So, we need to apply inverse transformation to get the actual count\n","# Predicted Count looks much better now\n","print ('Predicted Count', math.expm1(float(result)))\n","\n","print('\\n'.join([transform_data(sample_one),transform_data(sample_two)]))\n","\n","# Prediction for multiple observations in the same call\n","result = client.invoke_endpoint(EndpointName=endpoint_name,\n","                       Body=('\\n'.join([transform_data(sample_one),\n","                                        transform_data(sample_two)]).encode('utf-8')),\n","                       ContentType='text/csv')\n","\n","result = result['Body'].read().decode('utf-8')\n","\n","result\n","\n","# Batch Prediction\n","# Transform data and invoke prediction in specified batch sizes\n","def run_predictions(data, batch_size):\n","\n","    predictions = []\n","\n","    transformed_data = [transform_data(row.strip()) for row in data]\n","\n","    # Splitting using regular expression as xgboost 1-2-2 is returning\n","    # predicted values with inconsistent delimiters (comma, newline or both)\n","\n","    # pattern looks for one or more of non-numeric characters\n","    pattern = r'[^0-9.]+'\n","\n","    for i in range(0, len(data), batch_size):\n","\n","        print(i,i+batch_size)\n","\n","        result = client.invoke_endpoint(EndpointName=endpoint_name,\n","                       Body=('\\n'.join(transformed_data[i : i + batch_size]).encode('utf-8')),\n","                       ContentType='text/csv')\n","\n","        result = result['Body'].read().decode('utf-8')\n","        result = re.split(pattern,result)\n","\n","        predictions += [math.expm1(float(r)) for r in result if r != \"\"]\n","\n","    return predictions\n","\n","run_predictions([sample_one,sample_two,sample_three],10)\n","\n","# Run a batch prediction on Test.CSV File\n","# Read the file content\n","data = []\n","with open('test.csv','r') as f:\n","    # skip header\n","    f.readline()\n","    # Read remaining lines\n","    data = f.readlines()\n","\n","len(data)\n","\n","%%time\n","predictions = run_predictions(data,100)\n","\n","len(predictions),len(data)\n","\n","#Invoke as Microservices with Lambda\n","# Boto3 SageMaker Invoke Endpoint\n","# This example shows how to invoke SageMaker Endpoint from outside of AWS environment using Boto3 SDK\n","# Boto is the Amazon Web Services (AWS) SDK for Python\n","# https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n","\n","# Common Data Formats\n","# https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html\n","\n","# Endpoint: XGBoost - Kaggle Bike Rental - Regressor Trained in XGBoost Lectures\n","# Makesure Endpoint is deployed before running this example\n","#\n","# Reference:\n","#  https://github.com/awslabs/amazon-sagemaker-examples\n","\n","# NOTE: SageMaker SDK now requires additional permissions DescribeEndpoint, DescribeEndpointConfig in-addition to InvokeEndpoint\n","#   boto3 SDK requires just InvokeEndpoint permission.\n","#   Please update SageMakerInvokeEndpoint permissions to reflect this policy document:\n","#   Logon with my_admin account and update permissions (IAM->Policies->SageMakerInvokeEndpoint->Edit Policy)\n","#\n","{\n","    \"Version\": \"2012-10-17\",\n","    \"Statement\": [\n","        {\n","            \"Sid\": \"VisualEditor0\",\n","            \"Effect\": \"Allow\",\n","            \"Action\": [\n","                \"sagemaker:DescribeEndpointConfig\",\n","                \"sagemaker:DescribeEndpoint\",\n","                \"sagemaker:InvokeEndpoint\"\n","            ],\n","            \"Resource\": \"*\"\n","        }\n","    ]\n","}\n","\n","import boto3\n","import math\n","import dateutil\n","import json\n","import re\n","\n","# Establish a session with AWS\n","# Specify credentials and region to be used for this session.\n","# We will use a ml_user_predict credentials that has limited privileges\n","boto_session = boto3.Session(profile_name='ml_user_predict',region_name='us-east-1')\n","\n","# Acquire a SageMaker Runtime client for us-east-1 region\n","client = boto_session.client(service_name='sagemaker-runtime',region_name='us-east-1')\n","\n","# Specify Your Endpoint Name\n","endpoint_name = 'xgboost-biketrain-v1'\n","\n","# Raw Data\n","#datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n","# Actual=562\n","sample_one = ['2012-12-19 17:00:00',4,0,1,1,16.4,20.455,50,26.0027]\n","# Actual=569\n","sample_two = ['2012-12-19 18:00:00',4,0,1,1,15.58,19.695,50,23.9994]\n","# Actual=4\n","sample_three = ['2012-12-10 01:00:00',4,0,1,2,14.76,18.94,100,0]\n","\n","# Single Observation\n","request = {\n","    \"instances\": [\n","        # First instance.\n","        {\n","            \"features\": sample_one\n","        }\n","    ]\n","}\n","\n","print(json.dumps(request,indent=2))\n","\n","# Multiple Observations\n","request = {\n","    \"instances\": [\n","        # First instance.\n","        {\n","            \"features\": sample_one\n","        },\n","        # Second instance.\n","        {\n","            \"features\": sample_two\n","        },\n","        # Third instance.\n","        {\n","            \"features\": sample_three\n","        }\n","    ]\n","}\n","\n","print(json.dumps(request,indent=2))\n","\n","# Raw Data Structure:\n","# datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n","\n","# Model expects data in this format (it was trained with these features):\n","# season,holiday,workingday,weather,temp,atemp,humidity,windspeed,year,month,day,dayofweek,hour\n","\n","def transform_data(data):\n","    features = data.copy()\n","    # Extract year, month, day, dayofweek, hour\n","    dt = dateutil.parser.parse(features[0])\n","\n","    features.append(dt.year)\n","    features.append(dt.month)\n","    features.append(dt.day)\n","    features.append(dt.weekday())\n","    features.append(dt.hour)\n","\n","    # Return the transformed data. skip datetime field\n","    return ','.join([str(feature) for feature in features[1:]])\n","\n","print('Raw Data:\\n',sample_one)\n","print('Transformed Data:\\n',transform_data(sample_one))\n","\n","# Single with error\n","request = {\n","    \"instances\": [\n","        # First instance.\n","        {\n","            \"features\": [\"hi there\",0,2]\n","        }\n","    ]\n","}\n","\n","try:\n","    transformed_data = [transform_data(instance['features']) for instance in request[\"instances\"]]\n","except Exception as err:\n","    print('Error when transforming: {0}'.format(err))\n","\n","# Single Observation\n","request = {\n","    \"instances\": [\n","        # First instance.\n","        {\n","            \"features\": sample_one\n","        }\n","    ]\n","}\n","\n","# Let's invoke prediction now\n","result = client.invoke_endpoint(EndpointName=endpoint_name,\n","                       Body=transform_data(request['instances'][0]['features']).encode('utf-8'),\n","                       ContentType='text/csv')\n","\n","result = result['Body'].read().decode('utf-8')\n","\n","# Model was trained with log1p(count)\n","# So, we need to apply inverse transformation to get the actual count\n","# Predicted Count looks much better now\n","print ('Predicted Count', math.expm1(float(result)))\n","\n","# Multiple Observations\n","request = {\n","    \"instances\": [\n","        # First instance.\n","        {\n","            \"features\": sample_one\n","        },\n","        # Second instance.\n","        {\n","            \"features\": sample_two\n","        },\n","        # Third instance.\n","        {\n","            \"features\": sample_three\n","        }\n","    ]\n","}\n","\n","for instance in request[\"instances\"]:\n","    print(instance)\n","    print('Transformed:')\n","    print(' ', transform_data(instance['features']))\n","\n","# XGBoost accepts data in CSV. It does not support JSON.\n","# So, we need to submit the request in CSV format\n","# Prediction for multiple observations in the same call\n","result = client.invoke_endpoint(EndpointName=endpoint_name,\n","                       Body=('\\n'.join(\n","                           [transform_data(instance['features'])\n","                                for instance in request[\"instances\"]]).encode('utf-8')),\n","                       ContentType='text/csv')\n","\n","result = result['Body'].read().decode('utf-8')\n","\n","print(result)\n","\n","# Splitting using regular expression as xgboost 1-2-2 is returning\n","# predicted values with inconsistent delimiters (comma, newline or both)\n","\n","# pattern looks for one or more of non-numeric characters\n","pattern = r'[^0-9.]+'\n","result = re.split(pattern,result)\n","predictions = [math.expm1(float(r)) for r in result if r != \"\"]\n","\n","predictions\n","\n","#Invoke API Gateway\n","# Invoke API Gateway Endpoint\n","# This example shows how to invoke SageMaker Endpoint from outside of AWS environment using API Gateway\n","# Ref: https://stackoverflow.com/questions/17301938/making-a-request-to-a-restful-api-using-python\n","\n","# Common Data Formats\n","# https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html\n","\n","# Endpoint: XGBoost - Kaggle Bike Rental - Regressor Trained in XGBoost Lectures\n","# Makesure Endpoint is deployed before running this example\n","\n","import requests\n","import json\n","\n","# Update the URL to point to your API Gateway endpoint\n","url = 'https://bjygvuald0.execute-api.us-east-1.amazonaws.com/beta'\n","\n","# Raw Data\n","#datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n","# Actual=562\n","sample_one = ['2012-12-19 17:00:00',4,0,1,1,16.4,20.455,50,26.0027]\n","# Actual=569\n","sample_two = ['2012-12-19 18:00:00',4,0,1,1,15.58,19.695,50,23.9994]\n","# Actual=4\n","sample_three = ['2012-12-10 01:00:00',4,0,1,2,14.76,18.94,100,0]\n","\n","# Single Observation\n","request = {\n","    \"instances\": [\n","        {\n","            \"features\": sample_one\n","        }\n","    ]\n","}\n","\n","request\n","\n","response = requests.post(url, data=json.dumps(request))\n","result = response.json()\n","\n","if result['statusCode'] == 200:\n","    predictions = json.loads(result['body'])\n","    print('Predicted Count: ', predictions)\n","else:\n","    print('Error',result['statusCode'], result['body'])\n","\n","# Multiple Observations\n","request = {\n","    \"instances\": [\n","        # First instance.\n","        {\n","            \"features\": sample_one\n","        },\n","        # Second instance.\n","        {\n","            \"features\": sample_two\n","        },\n","        # Third instance.\n","        {\n","            \"features\": sample_three\n","        }\n","    ]\n","}\n","\n","response = requests.post(url, data=json.dumps(request))\n","\n","result = response.json()\n","\n","if result['statusCode'] == 200:\n","    predictions = json.loads(result['body'])\n","    print('Predicted Count: ', predictions)\n","else:\n","    print('Error',result['statusCode'], result['body'])\n","\n"],"metadata":{"id":"Sf-sUYO6XvLv"},"execution_count":null,"outputs":[]}]}